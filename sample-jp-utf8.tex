%%
% このファイルは，筑波大学情報学群情報メディア創成学類の
% 卒業研究論文本体のサンプルです．
% このファイルを書き換えて，この例と同じような書式の論文本体を
% LaTeXを使って作成することができます．
% 
% PC環境や，LaTeX環境の設定によっては漢字コードや改行コードを
% 変更する必要があります．
%%
\documentclass[a4paper,11pt]{jreport}

%%【PostScript, JPEG, PNG等の画像の貼り込み】
%% 利用するパッケージを選んでコメントアウトしてください．
%%
%% 推奨： graphicx パッケージ(本ファイルではこれを使用)
%%	動かない場合にいは \usepackage[dvipdfmx]{graphicx} のように dvi 変換コマンドを明示指定する．
%%
% \usepackage{graphicx} % for \includegraphics[width=3cm]{sample.eps}
% \usepackage[dvipdfm]{graphicx} % 画像の挿入に必要
\usepackage[dvipdfmx]{graphicx}
\usepackage[dvipdfmx]{color}
\usepackage{float} % 図の位置の強制指定に必要
%% 一応 OK (epsfig.sy)
% \usepackage{epsfig} % for \psfig{file=sample.eps,width=3cm}
%% 以下の２つの使用はあまり勧められない (epsf.sty, epsfbox.sty)
%\usepackage{epsf} % for \epsfile{file=sample.eps,scale=0.6}
%\usepackage{epsbox} % for \epsfile{file=sample.eps,scale=0.6}

\usepackage{times} % use Times Font instead of Computer Modern
\usepackage{mdframed}
\usepackage{latexsym} % 数式で使える記号を増やす
\usepackage{amsmath} % 数式の記述環境
\usepackage{fancyhdr} % ヘッダとフッタの設定に必要
\usepackage{algorithm,algpseudocode} % 疑似コードの記述に必要
\usepackage{array}
\usepackage{booktabs} % For formal tables
\usepackage{listings}

\setcounter{tocdepth}{3}	% 目次を3レベル (1.2.3) まで
\setcounter{secnumdepth}{3}	% 番号付けレベル． 3: \section まで 4: \subsubsection まで
\setcounter{page}{-1}

\setlength{\oddsidemargin}{0.1in}
\setlength{\evensidemargin}{0.1in} 
\setlength{\topmargin}{0in}
\setlength{\textwidth}{6in} 
%\setlength{\textheight}{10.1in}	% ページの縦幅を変更する場合には設定する
\setlength{\parskip}{0em}
\setlength{\topsep}{0em}

\definecolor{promptcolor}{rgb}{0.36, 0.54, 0.66}
\definecolor{responsecolor}{rgb}{0.0, 0.5, 0.0}

\lstdefinestyle{chatgptstyle}{
    basicstyle=\ttfamily\small,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    columns=flexible,
    backgroundcolor=\color{white},
    moredelim=[is][\textcolor{promptcolor}]{<<}{>>},
    moredelim=[is][\textcolor{responsecolor}]{[[}{]]},
}

%% タイトル生成用パッケージ(重要)
\usepackage{mast-jp-utf8}

%% タイトル
%% 【注意】タイトルの最後に\\ を入れるとエラーになります
\title{大規模言語モデルに基づく\\語用論的概念ラベリング}
%% 著者
\author{清野 駿}
%% 指導教員
\advisor{若林 啓}

%% 年月 (提出年月)
%% 年月は必要に応じて書き替えてください．
\majorfield{　} \yearandmonth{2024年 2月}



\begin{document}
\maketitle
\thispagestyle{empty}
\newpage

\thispagestyle{empty}
\vspace*{20pt plus 1fil}
\parindent=1zw
\noindent
%%
%% 論文の概要(Abstract)
%%
\begin{center}
{\bf 概要}
\vspace{5mm}
\end{center}
本研究は，多数の画像から特定の一枚を選ぶ際に役立つ自然言語ラベルを生成するための，
大規模言語モデルを用いた新しい手法を提案する．

近年，
大量のテキストデータの学習によって高品質なテキストを生成する大規模言語モデルの発展が目覚ましい．
大規模言語モデルは，テキストデータが用いられるすべての分野において活用される可能性を持っており，
大規模言語モデルの性能を向上させる試みは，それらの分野や産業界に対する大きな貢献になるといえる．
大規模言語モデルについて，モデルに入力するプロンプトの方式の工夫やモデル同士の対話により，
出力されるテキストの品質が向上することが知られている．
また，大規模言語モデルは，入力された画像に対して画像を説明するようなラベルの生成が可能である．
しかし，
複数の画像に対して画像同士を区別可能なラベルを生成する「画像情報ラベリング」に対しては，
大規模言語モデルの対話による出力品質の向上の検証は十分に行われていない．
加えて，画像情報ラベリングを解決するためのフレームワークとして語用論的アプローチによる手法が研究されているが，
ラベルの生成および評価モデルの学習コストが大きいという課題がある．

そのため本研究では，
大規模言語モデル同士の対話による性能向上が，画像情報ラベリングに対して有効であるかを検証する．
そのたに，既存の語用論的アプローチにおけるラベルの生成モデルと評価モデルに大規模言語モデルを導入した，
PCG(Pragmatic ChatGPT)を提案する．
実験では，提案手法とベースライン手法によって生成したラベルを用いて被験者実験を行い，
それぞれの手法によって生成されたラベルの品質を評価した．
実験の結果，一定の条件下において提案手法はベースライン手法を上回る傾向がみられた．
このことから，大規模言語モデル同士の対話による出力の品質情報は，
画像情報ラベリングに対しても有効に働く傾向にあることが示された．

%%%%%
\par
\vspace{0pt plus 1fil}
\newpage

\pagenumbering{roman} % I, II, III, IV 
\tableofcontents
\listoffigures
%\listoftables		% 本ファイルでは省略してある

%% ここから本文
\pagebreak \setcounter{page}{1}
\pagenumbering{arabic} % 1,2,3


\chapter{はじめに}

近年，膨大なデータと深層学習によって構築された，
事前学習済みのマルチモーダルなニューラルモデルである大規模言語モデルが注目を浴びている\cite{Yin2023}．
その活用はテキストデータを使用するすべての分野で行われ，
医療分野での医学的証拠の要約など，
大量にあるデータの抽出や構造化等で大きな貢献を果たしている\cite{Tang2023}．
そのため，大規模言語モデルの性能を向上させる試みは，それらの分野や産業界に対する大きな貢献になるといえる．
これらのモデルは，モデルの規模(パラメータ数)と訓練データの量を増やすことで，
その品質が向上し続ける傾向にある\cite{Devlin2019}．

大規模言語モデルについて，モデルに入力するプロンプトの方式の工夫やモデル同士の対話により，
出力されるテキストの品質が向上することが報告されている\cite{Wei2022, Zhou2022, Madaan2023, Zheng2023,LiRuosen2023}．
また，大規模言語モデルは，入力された画像に対して画像を説明するようなラベルの生成が可能である．
しかし，
複数の画像に対して画像同士を区別可能なラベルを生成する「画像情報ラベリング」に対しては，
大規模言語モデルの対話による出力品質の向上の検証は十分に行われていない．
加えて，
似た画像の区別を目的とした画像キャプション生成の手法の研究は行われている\cite{Vedantam2017,Cohn-Gordon2018,Nie2020,Andreas2016}が，
これらの研究では画像キャプション生成モデルの訓練を行っていることから，
大規模言語モデルを活用したキャプションの生成ができないという課題がある．

本研究の目的は，画像情報ラベリングにおいて大規模言語モデルの潜在能力を活用し，
加えて語用論的アプローチを組み合わせることにより，ラベリングの精度を向上させることにある．
語用論的アプローチとは，聞き手の解釈を考慮して誤解を避ける言語を生成する手法であり，
複数のテキスト生成タスクにおいて品質の向上をもたらすことが報告されている．
提案される手法は，
大規模言語モデルによって生成されたラベルの中から，誤解を引き起こしやすいものを特定し，
これを再び大規模言語モデルで修正するというアプローチに基づいている．

実験においては，複数の部屋の画像を用い，それぞれに対してラベルを生成した．
このラベルの評価にはAmazon Mechanical Turkを利用し，人間による誤解の程度を検証した．
実験結果より，テキスト長が短いラベルにおいて，提案手法によってラベルの品質が向上する傾向にあることが示された．
これは，大規模言語モデルの対話による性能向上が，
複数の画像に対する区別可能なラベルの生成において有効である可能性を示唆している．


% 関連研究
\chapter{関連研究}
本章では，本研究で扱う大規模言語モデルに関連する研究として，大規模言語モデルの性能向上技術について\ref{sec:fine_tune_llm}節で述べる．
加えて，本研究で扱う画像情報ラベリングタスクに関連する研究として，画像に基づくテキスト生成技術について\ref{sec:relwork_image_captioning}節で述べる．

\section{モデル同士の会話による大規模言語モデルの性能向上}
\label{sec:fine_tune_llm}
大規模言語モデルは様々な場面で活用され，その効果と課題について議論されている．
大規模言語モデルが持つ情報の抽出と要約の能力は，専門知識を要するケースにおいて課題があることが明らかになっている．
Tangら\cite{Tang2023}は，医療分野での医学的証拠の要約に大規模言語モデルを利用し，要約した文章に事実との不整合が発生したり，長文の出力で品質が低下するなどの限界があることを示した．

このような大規模言語モデルの性能限界を考慮して，大規模言語モデルの性能を向上させる研究が進んでいる．
近年，大規模言語モデルのプロンプトの工夫によって，出力の品質が向上することが明らかになっている．
思考過程のプロンプト化\cite{Wei2022}や，
全体の問題を簡単な部分問題に分割する手法\cite{Zhou2022}，
初期出力とその出力に対するフィードバックの反復による自己改善\cite{Madaan2023}，
そして出力内容をヒントとして元のプロンプトに追記する手法\cite{Zheng2023}などにより，
大規模言語モデルの出力の品質が向上することが示されている．

単一の大規模言語モデルの利用におけるプロンプトの工夫が研究される一方で，
異種類の大規模言語モデルを利用することによっても性能が向上することが明らかになっている．
Liら\cite{LiRuosen2023}は，「ピアランク(PR)」と「ピアディスカッション(PD)」という評価フレームワークを用いた異なる大規模言語モデル同士の議論によって，
より品質の高い回答を得ることが可能であることを示した．

これらのプロンプトの工夫や大規模言語モデル同士の会話による性能向上は，
様々なタスクにおいて有効に働くことが示されているが，
複数の画像に対して画像同士を区別可能なラベルを生成するタスクに対しての有効性は十分に検証されていない．
本研究では，画像情報ラベリングタスクに対する，大規模言語モデルの会話による性能向上の有効性について検証する．

\section{画像に基づくテキスト生成}
\label{sec:relwork_image_captioning}
これまで多くの画像キャプション生成技術が研究され，画像情報に基づく文書生成の分野の発展に寄与してきた \cite{Farhadi2010}．
画像キャプションの生成に対して深層学習を適用する手法では，画像の特徴に基づいて再帰型ニューラルネットワークがキャプションを生成するフレームワークが利用される\cite{Vinyals2017}．
このフレームワークでXuら\cite{Xu2015}は，視覚的注意メカニズムを備えたニューラルネットワークを用いて画像内の注目すべき点の情報を取得することで，画像キャプション生成の精度が向上することが示された．

さらに近年では，深層学習のモデル規模と訓練データをスケールアップすることで，ユーザーのテキストによる指定に対応して，画像からテキストを生成することが可能であることが明らかになった．
Liら\cite{Li2023}は，学習済みの画像特徴量抽出モデルとテキスト生成モデルの情報の受け渡しを行うQuerying Transformerの学習によって，画像に関するより高品質な説明や会話の生成ができることを示した．
GPT-4\cite{Bubeck2023}は，入力した画像の描画や，グラフに関する解釈が可能であることも示している．

一方近年，計算語用論と呼ばれる，聞き手の考えを考慮した話し手によるアプローチがされている \cite{Fried2023}．
特に，Rational Speech Acts (RSA)フレームワークが，代表的なモデルとして知られている \cite{Frank2012,Goodman2016}．
RSAフレームワークは，話し手モデルと聞き手モデルの双方が，相手の意図について再帰的に推論を行う過程を計算する．

これらの背景から，RSAフレームワークをテキスト生成に応用する手法が研究されており\cite{Fried2017}，
室内のナビゲーションタスクに計算語用論を導入する手法が提案されている\cite{Williams2015}．
SAILデータセットを使用した評価によって，語用論的アプローチがナビゲーションタスクの成功率の向上に寄与することが示された．

画像キャプション生成においても，語用論的アプローチの導入によって，似た画像の区別が可能なキャプションを生成する手法が研究されている \cite{Vedantam2017,Cohn-Gordon2018,Nie2020}．
Andreasら\cite{Andreas2016}は，画像の参照ゲームにおいて，聞き手の語解を最小限に抑えるための手法を提案した．

これらの計算語用論に基づく手法は，ニューラルネットワークによって構築された話し手モデルと聞き手モデルの学習を行っているため，高い品質の出力が可能な大規模言語モデルを活用することは難しい．
加えて，深層学習に基づく画像キャプショニングの手法は1枚か少数の画像に対するテキスト生成を想定しているため，
複数の画像に対して画像同士を区別するようなラベルの生成タスクに対する検証は十分にされていない．
本研究では，画像情報ラベリングにおいて，大規模学習済みモデルの語用論的アプローチによる利用によって，誤解可能性を考慮した画像情報ラベリングを実現する手法を検討する．

%提案手法
\chapter{提案手法}

\section{問題設定}

本研究では，入力された画像の集合に対して，各画像を誤解なく一意に識別可能なラベルを生成する問題に取り組む．

n枚の画像の集合を\(X = \{x_1, x_2, \ldots, x_n\}\)とする．\(X\)に対して付与されるラベルの集合を\(Y = \{y_1, y_2, \ldots, y_n\}\)とする．ラベル\(y_1, y_2, \ldots, y_n\)は画像\(x_1, x_2, \ldots, x_n\)にそれぞれ対応しており，各ラベルは対応する画像の特徴を説明するものとする．

ここで，ラベル\(y_r\)($r \in [1..n]$)と\(X\)を被験者に与えたとき，被験者が\(y_r\)に対応する画像\(x_r\)を選択しようとして，\(x_r\)以外を選択してしまう確率を誤解可能性\(p\)とする．

提案手法では，\(p\)が最小となるラベルの組み合わせ\(Y\)を生成することを目的とする．

\section{顕著性と識別性について}

ラベルの誤解可能性を決定づける要素として，「顕著性」と「識別性」を導入する．

顕著性は，ラベルが対応する画像の特徴をどれだけ正確に捉えて表現できているかを指すパラメータである．
一方，識別性は，ラベルに対応する画像がそれ以外の画像からどれだけ一意に識別可能かを指すパラメータである．

顕著性と識別性は互いに独立したパラメータである．
そのため，「顕著性は高いが識別性は低いラベル」等も存在する．

顕著性と識別性について具体的な例を示す．
図\ref{fig:white_bed}，\ref*{fig:pink_bed}が与えられた際に，
図\ref{fig:white_bed}を誤解なく一意に特定できるラベルを生成する，という問題を考える．
ここではラベルによって説明を行う画像をラベリング対象画像とする．
また，ラベリング画像とは別の画像，即ち誤解が生じぬように区別すべき画像を識別対象画像とする．

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/3-2_white_bed.jpg}
	\caption{ラベリング対象画像}
	\label{fig:white_bed}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/3-2_pink_bed.jpg}
	\caption{識別対象画像}
	\label{fig:pink_bed}
\end{figure}

この問題における，顕著性と識別性の有無を反映したラベルを，それぞれ以下に示す．

\paragraph*{顕著性が高く，識別性が低いラベル}
「天井に張り巡らされた木製の梁」．
ラベリング対象画像の説明は十分できているが，識別対象画像にも同様の特徴が含まれており，2画像の区別ができていない．
\paragraph*{顕著性が低く，識別性が高いラベル}
「暖炉のある部屋」．ラベリング対象画像のみに存在する特徴「暖炉」を表現することで，識別対象画像との区別に関してはできている．
しかし，ラベリング対象画像に含まれる特徴の中で特徴「暖炉」が画像を占める面積は小さく，
画像の特徴を十分に説明できているとは言えない．
\paragraph*{顕著性と識別性どちらも高いラベル}
「真っ白なベッドのある寝室」．
部屋の特長を十分に説明しており，かつ識別対象画像との区別ができる特徴「真っ白なベッド」を含んでいる．

\section{話し手モデルと聞き手モデル}

本研究では，語用論に基づくアプローチを用いて，ラベルの品質を改善する．
語用論は，言語交渉の過程において話し手と聞き手の意図の相互理解を探求する学問分野であり，
特に皮肉表現や比喩表現などの複雑な言語現象を研究対象としている．
本研究における語用論的アプローチは，話し手モデルと聞き手モデルの2つの対話モデルに基づいて構築される．
話し手モデルと聞き手モデルの双方向の対話を通じて，誤解のないラベルの生成を目指す．

話し手モデルの目的は，画像を正確に表現したラベルの生成である．
このモデルでは，画像の主要な特徴を十分に説明し，ラベルの曖昧性を排除することを重視する．

一方，聞き手モデルは，話し手モデルによって生成されたラベルが
誤解を招く可能性があるかを評価する．
このモデルはラベルが提供する情報に関して，
他のラベルとの間で起こり得る誤解の有無を検証し，
その結果を話し手モデルに対してフィードバックする．

% \section{ベースライン(ChatGPT手法)}
% プロンプトの説明中心
\section{Pragmatic ChatGPT(PCG)手法}
本研究では，前節における話し手モデルと聞き手モデルの両方にChatGPTを導入した，
Pragmatic ChatGPT(PCG)を提案する．

PCGの目的は，ラベルの識別性を高めることである．
PCGでは，話し手モデルによるラベルの生成と，聞き手モデルによるラベルの評価を行い，
この2つのモデルの処理の反復により，ラベルの誤解可能性を段階的に減少させる．

PCGにおけるラベル改善処理のプロセスを図\ref{fig:PCGflow}に示す．
ラベル改善処理では，初めに，話し手モデル1が対象画像に対する初期ラベルを生成する．
続いて，この初期ラベルに対し，聞き手モデルが改善に向けた具体的なヒントを提供する．
これらのヒントを基に，話し手モデル2は改良されたラベルを作成し，
それを再び聞き手モデルへと提出する．
このプロセスの繰り返しにより，ラベルの誤解可能性を徐々に低減させ，
ラベルの識別性を向上させることを目指す．
各サイクルは，ラベルの精度と明確性の向上に寄与し，
最終的にはより効果的な画像認識と解釈を可能にすることが期待される．

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/PCGflow.png}
	\caption{ラベル改善フロー}
	\label{fig:PCGflow}
\end{figure}

図\ref{fig:PCGflow}における，話し手モデル1(初期ラベル生成フロー)の処理の流れを図\ref{fig:PCGspeaker1}に示す．
話し手モデル1は，入力された画像の集合に対して，各画像の特徴を説明するようなキャプション生成を行う．
このとき，話し手モデル2にあるような，ラベル改善のヒントを踏まえた生成などは行わず，
純粋なキャプショニングによってラベルを生成する．

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/PCGspeaker1.png}
	\caption{話し手モデル1(初期ラベル生成フロー)}
	\label{fig:PCGspeaker1}
\end{figure}

話し手モデル１で行っている，初期ラベルを生成する処理を行うChatGPTプロンプトを図\ref{fig:prompt_speaker1}に示す．


\begin{figure}[h]
  \centering
  \begin{mdframed}
  \begin{lstlisting}[style=chatgptstyle]
    [[Prompt: Now input 10 images. For each group of images you have entered, output a sentence that captures three characteristics of the images, one sentence for each image, so that each image can be distinguished. Never output in any format other than the specified format. The output format should be as follows. 1.[Caption for image 1] 2.[Caption for image 2] ~~ 10.[Caption for image 10] Example: 1. A living room, white walls, wallpaper with pink and black.]]

    <<Responce: Please upload the 10 images you'd like me to analyze.>>

    [[Prompt: {Image URLs}]]
  \end{lstlisting}
  \end{mdframed}
  \caption{初期ラベル生成プロンプト．画像10枚が入力され，各画像を区別するようにラベルを生成する．
  出力は"1.[Caption for image 1] 2.[Caption for image 2] ~~ 10.[Caption for image 10]"の形式で行われる．}
  \label{fig:prompt_speaker1}
\end{figure}

出力されるラベル例を図\ref{fig:firstlabel_example}に示す．

\begin{figure}[H]
  \begin{mdframed}[linewidth=1pt]
    1. A bedroom with large windows offering a view, a cozy sitting area, and a neutral color palette.
  \end{mdframed}
  \caption{出力されるラベルの例}
  \label{fig:firstlabel_example}
\end{figure}



次に図\ref{fig:PCGflow}における聞き手モデル(ヒント生成フロー)の処理の流れを図\ref{fig:PCGlistener}に示す．
聞き手モデルは，入力された画像の集合と，話し手モデルによって生成されたラベルから，
各ラベルの誤解可能性について評価を行い，ラベルの改善に役立つヒントを出力する．

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/PCGlistener.png}
	\caption{聞き手モデル(ヒント生成フロー)}
	\label{fig:PCGlistener}
\end{figure}

聞き手モデルが生成するヒントは，誤解する可能性の高い2つのラベルの組み合わせを示す形式をとる．
例えば，ラベル1とラベル3に同じ単語が含まれており，
それぞれのラベルに対応する画像にも同様の特徴が含まれていた場合，
そのラベル組の識別性は互いに低くなると考えられる．
そのような場合，そのラベルの組の誤解可能性が高いとして，
改善すべきラベル組はラベル1とラベル3である，といったようにラベル改善のヒントを出力する．

聞き手モデルで行っている，ラベル改善のヒントを生成する処理を行うChatGPTプロンプトを図\ref{fig:prompt_listener}に示す．

\begin{figure}[h]
  \centering
  \begin{mdframed}
  \begin{lstlisting}[style=chatgptstyle]
    [[Prompt: Now enter 10 images and 10 captions corresponding to each image with 10 sentences.For each input image and caption group, verify that each caption can distinguish each image without misunderstanding. Then, output the combination of captions that you think is most likely to cause misunderstanding in the form of [Caption 1 and Caption 2]. The input format is as follows. 10 image files: to be entered later. Caption 10 sentences: [caption number]. [Caption text]. As an example, the following will be input. 1. red bed room, white curtains, yellow modern lamps 2. ~~ omitted ~~ 10. red bed room, blue chairs, red wall.  Output format to be described later]]

    <<Responce: Please enter 10 pairs of image files and their corresponding captions. For each image, we will review the potentially misleading captions and identify the most confusing caption combinations. Once you have provided us with the images and captions, we will begin our analysis.>>

    [[Prompt: {Image URLs}]]

    <<Responce: To proceed with the analysis, please provide the captions for each image. Once I have the captions, I will be able to review them in the context of the images and identify any potential for confusion.>>

    [[Prompt: 
    {labels}

    These are the captions for each image. Never output in any format other than the specified format. Output should be in accordance with the following output format. hint 1. [caption n] and [caption n]: [reference to possible misunderstanding]. hint2. [caption n] and [caption n]: [reference to possible misunderstanding]. ~~ hint n. [caption n] and [caption n]: [reference to possible misunderstanding]. Example: hint 1. Caption 1 and Caption 10: Both highlight colors (pink and red) and may refer to the art piece.
    
    ]]
  \end{lstlisting}
  \end{mdframed}
  \caption{ラベル改善のヒントを生成する処理を行うプロンプト．10枚の画像と10文のラベルが入力され，語解の原因となるラベルの組み合わせを出力する．
  出力は"1. [caption n] and [caption n]: [reference to possible misunderstanding]. hint2. [caption n] and [caption n]: [reference to possible misunderstanding]. ~~ hint n. [caption n] and [caption n]: [reference to possible misunderstanding]."の形式で行われる．}
  \label{fig:prompt_listener}
\end{figure}

出力されるヒントの例を図\ref{fig:hint_example}に示す．

\begin{figure}[H]
  \begin{mdframed}[linewidth=1pt]
    hint 1. Caption 3 and Caption 10: Both mention contemporary elements with large windows and a spacious layout, which could refer to modern architecture with a minimalist theme.
  \end{mdframed}
  \caption{出力されるラベル改善のヒントの例}
  \label{fig:hint_example}
\end{figure}

最後に図\ref{fig:PCGflow}における話し手モデル2(ラベル改善フロー)の処理の流れを図\ref{fig:PCGspeaker2}に示す．
話し手モデル2では，画像の集合，既に生成されているラベル，聞き手モデルによって生成されたラベル改善のヒントをもとに，
新たなラベルを生成する．
このとき，ラベル改善のヒントをもとにして既に生成されているラベルの改善を行うことで，
よりラベルの誤解可能性を小さくすることを狙う．

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/PCGspeaker2.png}
	\caption{話し手モデル２(ラベル改善フロー)}
	\label{fig:PCGspeaker2}
\end{figure}

話し手モデル２で行っている，ヒントに基づいて改善したラベルを生成する処理を行うChatGPTプロンプトを図\ref{fig:prompt_speaker2}に示す．

\begin{figure}[h]
  \centering
  \begin{mdframed}
  \begin{lstlisting}[style=chatgptstyle]
    [[Prompt: 
    Now enter 10 images and 10 captions corresponding to each image.In addition, enter a hint referring to possible misunderstandings about each caption.For each group of images you have entered, improve the captions based on the hints and output one sentence for each image, capturing three characteristics of each image that distinguish each image.Input 10 images, 10 caption sentences, and the hints in that order.
    ]]

    <<Responce: 
    Understood. Please enter the 10 images, a caption for each, and a hint to prevent misunderstandings about each caption in order. We will then provide improved captions that capture the characteristics of each image based on the hints.
    >>

    [[Prompt: {Image URLs}]]

    <<Responce: 
    Continue by providing a caption for each image and tips to prevent misunderstandings about that caption. Based on this information, create an improved caption that captures three features of the image.
    >>

    [[Prompt: First, the captions corresponding to each image are as follows.

    {labels}

    Never output in any format other than the specified format. Output should be in accordance with the following output format. hint 1. [caption n] and [caption n]: [reference to possible misunderstanding]. hint2. [caption n] and [caption n]: [reference to possible misunderstanding]. ~~ hint n. [caption n] and [caption n]: [reference to possible misunderstanding]. Example: hint 1. Caption 1 and Caption 10: Both highlight colors (pink and red) and may refer to the art piece. Below are some tips on misconceptions to avoid." + 

    {hints}

    Based on these tips, improve the original caption so that the caption captures three features of each image that distinguish each image. Never output in any format other than the specified format. The output format should be as follows. 1.[Caption for image 1] 2.[Caption for image 2] ~~ 10.[Caption for image 10] Example: 1. Example: 1. A living room with wood floors, white walls and ceiling, and wallpaper with pink and black patterns.
    ]]
  \end{lstlisting}
  \end{mdframed}
  \caption{ヒントに基づいて改善したラベルを生成する処理を行うプロンプト．10枚の画像，10文のラベル，ラベル改善のヒントが入力され，ヒントに基づいて改善を行ったラベルを10文出力する．
  出力は"1.[Caption for image 1] 2.[Caption for image 2] ~~ 10.[Caption for image 10]"の形式で行われる．}
  \label{fig:prompt_speaker2}
\end{figure}

画像の集合をX，n回目の改善サイクルにおけるラベルをYn，n回目の改善サイクルにおけるヒントをH，
改善サイクルを回す回数をmとして，
ここまでのラベル改善処理をまとめた流れは以下の通りである．

\begin{enumerate}
  \item ChatGPTに対して画像の集合Xを入力する．Xに含まれる各画像に対して，それぞれ誤解なく画像を区別できるラベルを出力するように，プロンプトを入力する．ここで出力されたラベルの集合をY1とする．
  \item ラベルの集合Y1と画像の集合XをChatGPTに入力する．Y1に含まれる各ラベルについて誤解可能性を分析し，ラベル改善のヒントを出力するように，プロンプトを入力する．出力されたラベル改善のヒントをH1とする．
  \item ChatGPTに対して画像の集合X，ラベル改善のヒントH1，ラベルの集合Y1を入力する．H1を参考にして，Y1を改善した新たなラベルを出力するように，プロンプトを入力する．ここで出力されたラベルの集合をY2とする．
  \item 2と3を繰り返して，最終的に得られたラベルの集合Ym(mは任意の自然数)を，誤解可能性の低いラベルとして提出する．
\end{enumerate}


% 実験
\chapter{評価実験}

\section{実験設定}

本実験では，各手法の性能を比較するため，生成されたラベルの誤解可能性を検証する．被験者に複数の画像と1つのラベルを与え，対応する画像を選択できるかを調査する．

実験全体の流れをまとめた図を図\ref{fig:flow_example}に示す．図の左部は手法によるラベルの評価部分で，右部は被験者実験によるラベルの評価部分である．

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/flow.png}
  \caption{実験の流れ}
  \label{fig:flow_example}
\end{figure}


ラベルの生成には以下の2つの手法を用いる．
\begin{enumerate}
    \item ChatGPTを用いたベースライン手法
    \item Pragmatic ChatGPT
\end{enumerate}

ChatGPTを用いたベースライン手法は，ChatGPTを用いて画像に対して1対1でラベルを生成する単純な手法である．ChatGPTを用いたベースライン手法を評価基準とし，これとの比較により提案手法の評価を行う．ベースライン手法の詳細な処理内容は4.2節で述べる．

また，実用性と汎用性の評価のために，それぞれの手法でラベル長を制限することによる以下の2種類の出力方法でラベル生成を行い，被験者実験による手法の評価に用いる．

\begin{enumerate}
    \item 通常出力(normal output)：ラベル長に制限を設けずに出力
    \item 短出力(minimal output)：ラベル長を制限(例：10単語以内)して出力
\end{enumerate}

ラベルの生成元となる画像には，Matterport3Dデータセットを使用する．データセット内に含まれる，同一地点から撮影した部屋の画像を結合することによってパノラマ画像を得る．パノラマ画像を1つの部屋を表す概念として扱い，いくつかの部屋のパノラマ画像に対してラベリングを行う．データセットの詳細は4.1.1節で述べる．

被験者実験はAmazon Mechanical Turk(AMT)を使用して行う．AMTはマイクロタスク型のクラウドソーシングサービスである．ワーカーに対してタスクをリクエストし，ワーカーがアクションを行い，その結果を回収する．

本実験では，ワーカーに複数の画像とラベルを与え，ラベルに紐づく画像を選択してもらう．AMTタスクの詳細は4.1.2節で述べる．

\subsection{データセット}

実験では，Matterport3Dデータセット\footnote{https://niessner.github.io/Matterport/}を使用する．Matterport3Dデータセットは，90の建物から成る，194,400枚のRGB-D画像による大規模なRGB-Dデータセットである．

実験では，データセットに含まれる画像に対してパノラマ画像化処理を行った後，寝室を撮影した画像10枚を抽出して実験に使用した．
パノラマ画像化処理について，詳細を述べる．
データセットは，上下と水平方向の合わせて3回，それを60度ずつ横方向にずらして6回，計18回の撮影を同一の地点で行った画像群で構成されている．実験では，同一地点で撮影した18枚の画像を適切な位置で結合する処理を，すべての画像に対して行うことで，撮影地点ごとにパノラマ画像を作成した．

生成したパノラマ画像の例を図\ref{fig:panorama_example}に示す．

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/panorama.jpeg}
  \caption{同一地点で撮影した画像18枚を結合したパノラマ画像}
  \label{fig:panorama_example}
\end{figure}

実験では，パノラマ画像の中から寝室を撮影した画像10枚を抽出して使用した．

\paragraph{パノラマ画像を実験で使用する理由}
ラベリング対象とする画像をパノラマ画像とする理由は，画像に含まれる特徴量を均一化するためである．データセットに含まれる画像のうち，家具や照明など部屋全体の特徴を捉えた画像はラベリングの対象として適切だが，壁のみが写っている等の部屋の特徴を含まない画像はラベリングの対象として適切ではない．このような，画像に含まれる特徴量の不均一によって引き起こされる意図的ではないラベリング精度の低下を回避するために，事前にパノラマ画像化処理を行い画像の特徴量を均一化する．

\paragraph{寝室の画像のみを抽出する理由}
ラベリング対象とする画像を寝室だけに制限する理由は，実験の難易度を適切に設定するためである．実験で使用するデータセットに含まれる画像の部屋の種類はキッチンやバスルーム，リビングなど複数存在している．これらの異なる種類の部屋の画像に対してラベリングを行った場合，「調理器具」「ベッド」などの一つ単語で各画像を容易に区別できる可能性がある．このような，画像同士の識別性を不用意に向上させかねない要因を排除するために，データセットから寝室の画像のみを抽出して実験に使用した．

\subsection{Amazon Mechanical Turk：AMTの概要}

Amazon Mechanical Turk(AMT)はマイクロタスク型に特化したクラウドソーシングサービスである．ワーカに対して何らかのタスク(アンケート，データラベリングなど)をリクエストし，ワーカがタスクに対して何らかのアクション(アンケートに回答する，データにラベルを付与するなど)を行い，ワーカが入力したデータを回収することで，問題を解決する．

実験では，4枚の画像からラベルに対応する画像を選ぶタスクを100回実行し，その正答率を手法ごとに比較する．

実験で行うAMTのタスクの設定を以下に示す．

\begin{itemize}
  \item ワーカに対して4枚の画像とラベルを与える．
  \item ワーカは，ラベルに紐づくと考えられる画像を一枚選択する．
\end{itemize}

AMTタスクの例を図\ref{fig:amt_example}に示す．

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/amt.png}
  \caption{被験者に表示されるAMTタスクの例}
  \label{fig:amt_example}
\end{figure}

手法ごとに，正解画像，ラベル，被りなくランダムに抽出した3枚の不正解画像の組み合わせによって作成したタスクを100回実行する．

タスクの作成手順の詳細は以下の通りである．
\begin{enumerate}
  \item 10枚の画像と，各画像に対応する10個のラベルが手法によって生成される．
  \item ある画像が正解となるタスクを10タスク作成する．このとき，不正解となる3つの画像は，ランダムに被りなく抽出して組み合わせる．
  \item 10枚の画像それぞれに対して2の処理を行い，100通りの組み合わせのタスクを作成する．
\end{enumerate}

\subsection{比較手法}

実験では，4種類の方法で出力したラベルを使用して比較実験を行う．

ラベルの生成には以下の2つの手法を用いる．ベースライン手法との比較により，提案手法の評価を行う．
\begin{enumerate}
    \item ChatGPT(ベースライン手法)
    \item Pragmatic ChatGPT(PCG，提案手法)
\end{enumerate}

さらに，手法の一般性と実用性の評価のために，各手法で以下の2種類の出力方法でラベルを生成する．
\begin{enumerate}
  \item 通常出力(normal output)：ラベル長に制限をかけずに出力
  \item 短出力(minimal output)：ラベル長を10単語以内に制限し出力
\end{enumerate}

短出力では，図\ref{fig:limiting_prompt}に示すプロンプトをラベル出力処理部分に追加することで，ラベル長を10単語以内に収めるように制御する．

\begin{figure}[H]
\begin{mdframed}[linewidth=1pt]
Sentences must be output within 10 words.
\end{mdframed}
\caption{ラベル長に10単語以内の制限を設けるプロンプト}
\label{fig:limiting_prompt}
\end{figure}


ラベル長の制限は，手法の実用性を検証するために行う．無制限に長いラベルを生成することにより，理論上誤解を排除することは可能であるが，これは実用的ではない．従って，実験では，短いラベルによる誤解の可能性を低減できるかどうかについても検証を行う．これは，ラベルの長さが誤解可能性に与える影響を評価するために不可欠である．実際の応用においては，簡潔でありながら効果的なラベルが求められるため，短いラベルにおける性能評価は手法の実用性と妥当性の検証という点で重要である．

2手法それぞれで通常出力と短出力のラベルを生成し，合計4種類のラベル群のセットで被験者実験を行う．実験結果では，各手法で生成したラベル群の平均単語数もあわせて示す．

ベースライン手法によって生成されたラベルの例を図\ref{fig:label_example}に示す．

\begin{figure}[H]
\begin{mdframed}[linewidth=1pt]
Bright bedroom with large windows, modern decor, scenic view.
\end{mdframed}
\caption{出力されるラベルの例}
\label{fig:label_example}
\end{figure}

このようなラベルの集合を各手法で生成し実験に使用する．

実験で比較する各手法の名前と内容を箇条書きで説明する．

\begin{enumerate}
  \item ChatGPT
  \begin{itemize}
  \item ChatGPTを使用して，1枚の画像を入力して1つのラベルを出力する手法．以下のプロンプトを与えて，画像に対するラベルを出力する．
  \begin{figure}[H]
    \begin{mdframed}[linewidth=1pt]
      Generate a one-sentence description that captures only three features of the room you have provided.
    \end{mdframed}
    \caption{ベースライン手法で使用するプロンプト}
    \label{fig:baseline_prompt}
  \end{figure}
\end{itemize}
  
  \item Pragmatic ChatGPT(PCG)
  \begin{itemize}
    \item ChatGPTに「話し手モデル」「聞き手モデル」を導入し，ラベル改善サイクルを10回行う手法．
  \end{itemize}
    
  \item ChatGPT minimal
  \begin{itemize}
    \item ChatGPTに単語長10単語までの制約を加えた手法．\ref{fig:limiting_prompt}のプロンプトを，ラベル出力部分に追加する．
  \end{itemize}
  
  \item Pragmatic ChatGPT minimal
  \begin{itemize}
    \item Pragmatic ChatGPTに単語長10単語までの制約を加えた手法．\ref{fig:limiting_prompt}のプロンプトを，ラベル出力部分に追加する．
  \end{itemize}
  
\end{enumerate}

\section{実験結果と考察}

\subsection{実験結果}

表\ref{tab:accuracy_result}は，手法ごとのAMTタスクの正答率と，タスクに使用したラベルの平均単語数を示している．
ChatGPTおよびChatGPT minimalがベースライン手法，Pragmatic ChatGPTおよびPragmatic ChatGPT minimalが提案手法である．

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\hline
手法 & 正答率 & 平均単語数 \\
\hline
ChatGPT & 0.43 & 26.9 \\
Pragmatic ChatGPT & 0.40 & 20.7 \\
ChatGPT minimal & 0.31 & 7.5 \\
Pragmatic ChatGPT minimal & 0.33 & 8.1 \\
\hline
\end{tabular}
\caption{手法ごとのAMTタスクの正答率と平均単語数}
\label{tab:accuracy_result}
\end{table}

表\ref{tab:accuracy_result}より，ラベル長を制限した場合(ChatGPT minimalとPragmatic ChatGPT minimal)，提案手法はベースライン手法の正答率を上回った．

ラベル長に制限が無い場合，提案手法はベースラインの正答率を下回った．

また，すべての比較手法において，平均単語数が高いほど正答率が高くなる傾向がみられた．

\subsection{考察}
本研究における提案手法は，ラベル長に制限がある場合に出力の品質が向上する傾向にあることが示された．
本考察における前提として，画像情報ラベリングというタスクにおけるラベルの特徴として，識別性と顕著性という概念を定義する．
識別性はラベルの情報によって各画像をどれだけ区別できるかどうかというパラメータで，
顕著性はラベルが対応する画像に含まれる特徴をどれだけ表現できているかというパラメータである．

ラベル長が限定される状況では，画像の多様な特徴を一つのラベルで表現することには限界があるため，
ラベルの顕著性を向上させるだけでは画像の区別が困難であると考えられる．
つまり，ラベル長が短いケースでは，ラベルの識別性を向上させることが正答率の向上につながると考えられる．
この点において，PCGのプロンプトではラベルの識別性を高める方向での出力品質の向上を目指していたため，
ベースライン手法よりも高い正答率を達成したと考えられる．

一方で，ラベル長に制限がない場合の提案手法の性能については，
いくつかの問題点が浮き彫りになった．
無制限のラベル長では，各画像を包括的に表現することが可能であり，
その場合，ラベルの顕著性を高めることがより重要となる．
しかし，提案手法が依然として識別性の向上に重点を置いているため，
この状況には適切に対応できていない．
この結果，ベースライン手法と比較して，提案手法の正答率は低下することが観察された．

これらの考察から，
より短いラベルの生成を目指す画像情報ラベリングに対して，
大規模言語モデルの会話による性能向上を適用することで，
より品質の高いラベルを生成できる傾向にあることが示された．
これは，大規模言語モデルの会話による性能向上が，
画像情報ラベリングに対しても一定の条件下において有効である可能性を示している．

また，提案手法の改善策として，
ラベルの顕著性と識別性のバランスを調整する機構の導入が提案される．
具体的には，ラベルが長い場合は顕著性を，短い場合は識別性を優先させることで，
ラベル長に応じた適切なラベル生成を行う．この改善により，ラベル長に制限がない状況においても，誤解可能性が低く，
より効果的なラベル生成が期待できる．

% 結論
\chapter{おわりに}

本研究では，大規模言語モデルの対話による性能向上を，画像キャプショニングに適用するための手法を提案した．
この研究の目的は，複数の画像同士を区別できるようなラベルの生成に対して
大規模言語モデル同士の対話フレームワークを用いることで，
出力するラベルの品質を向上させることにある．

提案手法であるPCGは，語用論的アプローチに基づく対話フレームワークの「話し手モデル」「聞き手モデル」に
大規模言語モデルを導入し，ラベルの誤解可能性を低減することでより効果的な画像識別を目指した．
話し手モデルが作成したラベルを聞き手モデルが評価し，
そのフィードバックに基づいてラベルを再生成するプロセスを繰り返すことで，
より適切なラベルの生成を目指す．
これにより，生成されたラベルが画像を一意に特定できるものであることが期待される．

実験では，生成したラベルの評価にAmazon Mechanical Turkを利用し，
生成されたラベルが人間にとってどの程度理解しやすいかを検証した．
その結果，提案手法によって生成されたラベルは，ラベル長に10単語以内の制約がある場合において，
ベースライン手法よりも正答率が高くなる傾向にあることが示された．
これは，提案手法が識別性を高める方向で効果を発揮することを示唆している．
しかし，ラベル長に制限がない場合には，提案手法の性能はベースラインを下回る結果となった．
これは，無制限のラベル長ではラベルの顕著性を向上させることが重要であり，
提案手法がこの点で十分な対応を行えていない可能性を示唆している．
これらのことから，大規模言語モデルの対話による性能向上が，
画像情報ラベリングに対して一定の条件下において有効である可能性が示された．

今後の展望として，より多様な画像やシーンに対するラベル生成の実施と，提案手法の汎用性の評価が挙げられる．
加えて，提案手法のさらなる改善が考えられる．
具体的には，ラベル長に応じて顕著性と識別性のバランスを調整する機構を導入することで，
ラベル生成の精度をさらに向上させることが可能であると考えられる．
この改善により，ラベル長が長い場合でも高品質なラベリングを実現し，実用性の高い応用が期待される．
これらの課題を解決することで，本手法はさらに広範な応用が可能となると考えられる．

\chapter*{謝辞}
\addcontentsline{toc}{chapter}{\numberline{}謝辞}	% 目次で左詰めするなら \numberline{} を削除する．

本研究の一部は，JST CREST (\#JPMJCR22M2) の助成によって行われた．

\newpage

\addcontentsline{toc}{chapter}{\numberline{}参考文献}	% 目次で左詰めするなら \numberline{} を削除する．
\renewcommand{\bibname}{参考文献}

\bibliographystyle{junsrt}
\bibliography{reference}

\end{document}
